# Meilisearch Crawler

Ce projet est un crawler web asynchrone et performant, con√ßu pour peupler une instance Meilisearch avec le contenu de divers sites web. Il sert de compagnon au projet [KidSearch](https://github.com/laurentftech/kidsearch), un moteur de recherche s√©curis√© pour les enfants.

Le crawler est configurable via un simple fichier YAML (`sites.yml`) et prend en charge √† la fois les pages HTML et les API JSON comme sources de donn√©es.

## Fonctionnalit√©s

- **Asynchrone & Parall√®le**: Con√ßu avec `asyncio` et `aiohttp` pour un crawl simultan√© √† haute vitesse.
- **Tableau de bord interactif**: Une interface web bas√©e sur Streamlit pour surveiller, contr√¥ler et configurer le crawler en temps r√©el.
- **Crawl multi-sites**: Explore plusieurs sites web d√©finis dans un unique fichier `sites.yml`.
- **Sources flexibles**: Prend en charge les sites web HTML standards et les API JSON.
- **Indexation incr√©mentielle**: Utilise un cache local pour ne r√©indexer que les pages qui ont chang√© depuis le dernier crawl.
- **Reprise du crawl**: Reprend automatiquement un crawl qui a √©t√© arr√™t√© par une limite de pages.
- **Extraction de contenu intelligente**: Utilise `trafilatura` pour une d√©tection robuste du contenu principal.
- **D√©tection de la langue**: D√©tecte automatiquement la langue des pages.
- **Respect de `robots.txt`**: Suit les protocoles d'exclusion standards.
- **Exclusions globales et par site**: D√©finissez des motifs d'URL globaux et sp√©cifiques au site √† ignorer.
- **CLI avanc√©e**: Options de ligne de commande puissantes pour un contr√¥le pr√©cis.

![screenshot_dashboard.png](media/screenshot_dashboard_fr.png)

## Pr√©requis

- Python 3.8+
- Une instance Meilisearch en cours d'ex√©cution (v1.0 ou sup√©rieure).

## 1. Configuration de Meilisearch

Ce crawler a besoin d'une instance Meilisearch pour y envoyer ses donn√©es. La mani√®re la plus simple d'en obtenir une est avec Docker.

1.  **Installez Meilisearch**: Suivez le guide de d√©marrage rapide officiel de Meilisearch.
2.  **Lancez Meilisearch avec une cl√© principale**:
    ```bash
    docker run -it --rm \
      -p 7700:7700 \
      -e MEILI_MASTER_KEY='une_cle_maitre_longue_et_securisee' \
      -v $(pwd)/meili_data:/meili_data \
      ghcr.io/meilisearch/meilisearch:latest
    ```
3.  **Obtenez votre URL et votre cl√© API**:
    -   **URL**: `http://localhost:7700`
    -   **Cl√© API**: La `MEILI_MASTER_KEY` que vous avez d√©finie.

## 2. Configuration du Crawler

1.  **Clonez le d√©p√¥t**:
    ```bash
    git clone https://github.com/laurentftech/MeilisearchCrawler.git
    cd MeilisearchCrawler
    ```

2.  **Cr√©ez et activez un environnement virtuel**:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Installez les d√©pendances**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configurez les variables d'environnement**:
    Copiez le fichier d'exemple et modifiez-le avec vos informations d'identification Meilisearch.
    ```bash
    cp .env.example .env
    ```
    Maintenant, ouvrez `.env` et remplissez vos `MEILI_URL` et `MEILI_KEY`.

5.  **Configurez les sites √† crawler**:
    Copiez le fichier d'exemple des sites.
    ```bash
    cp config/sites.yml.example config/sites.yml
    ```
    Vous pouvez maintenant modifier `config/sites.yml` pour ajouter les sites que vous souhaitez indexer.

## 3. Lancer le Crawler

Vous pouvez lancer le crawler via la ligne de commande ou le tableau de bord interactif.

### Interface en Ligne de Commande

Ex√©cutez simplement le script `crawler.py`:

```sh
python crawler.py # Lance un crawl incr√©mentiel sur tous les sites
```

**Options courantes:**

-   `--force`: Force une r√©indexation compl√®te de toutes les pages, en ignorant le cache.
-   `--site "Nom du Site"`: N'explore que le site sp√©cifi√©.
-   `--workers N`: D√©finit le nombre de requ√™tes parall√®les (ex: `--workers 10`).
-   `--stats-only`: Affiche les statistiques du cache sans lancer de crawl.

**Exemple:**
```sh
# Force une r√©indexation de "BBC Bitesize" avec 10 workers parall√®les
python crawler.py --force --site "BBC Bitesize" --workers 10
```

### Tableau de Bord Interactif

Le projet inclut un tableau de bord web pour surveiller et contr√¥ler le crawler en temps r√©el.

**Comment le lancer:**

1.  Depuis la racine du projet, ex√©cutez la commande suivante:
    ```bash
    streamlit run dashboard/dashboard.py
    ```
2.  Ouvrez votre navigateur web √† l'URL locale fournie par Streamlit (g√©n√©ralement `http://localhost:8501`).

**Fonctionnalit√©s:**

-   **üè† Vue d'ensemble**: Un r√©sum√© en temps r√©el du crawl en cours (pages index√©es, sites explor√©s, erreurs, graphiques de progression).
-   **üîß Contr√¥les**: D√©marrez ou arr√™tez le crawler, s√©lectionnez un site sp√©cifique, forcez une r√©indexation et videz le cache.
-   **üîç Recherche**: Une interface de recherche pour tester des requ√™tes directement sur votre index Meilisearch.
-   **üìä Statistiques**: Des statistiques d√©taill√©es sur votre index Meilisearch, y compris la distribution des documents par site.
-   **üå≥ Arbre des Pages**: Visualisez la structure, la fra√Æcheur des pages index√©es et les pages en attente de crawl.
-   **‚öôÔ∏è Configuration**: Un √©diteur interactif pour voir et modifier le fichier de configuration `sites.yml`.
-   **ü™µ Logs**: Une vue en direct du fichier de log du crawler.

## 4. Configuration de `sites.yml`

Le fichier `config/sites.yml` vous permet de d√©finir une liste de sites √† crawler. Chaque site est un objet avec les propri√©t√©s suivantes:

- `name`: (String) Le nom du site, utilis√© pour le filtrage dans Meilisearch.
- `crawl`: (String) L'URL de d√©part pour le crawl.
- `type`: (String) Le type de contenu. Peut √™tre `html` ou `json`.
- `delay`: (Float, optionnel) D√©lai minimum en secondes entre les requ√™tes pour ce site.
- `max_pages`: (Integer) Le nombre maximum de pages √† crawler. Mettre `0` ou omettre pour ne pas avoir de limite.
- `depth`: (Integer) La profondeur maximale pour suivre les liens √† partir de l'URL de d√©part.
- `selector`: (String, optionnel) Pour les sites HTML, un s√©lecteur CSS sp√©cifique (ex: `.main-article`) pour cibler la zone de contenu principal.
- `lang`: (String, optionnel) Pour les sources JSON, sp√©cifie la langue du contenu (ex: "en", "fr").
- `exclude`: (Liste de cha√Ænes) Une liste de motifs d'URL √† ignorer compl√®tement.
- `no_index`: (Liste de cha√Ænes) Une liste de motifs d'URL √† visiter pour d√©couvrir des liens mais √† ne pas indexer.

### Configuration sp√©cifique au type JSON

Si `type` est `json`, vous devez √©galement fournir un objet `json` avec le mappage suivant:

- `root`: La cl√© dans la r√©ponse JSON qui contient la liste des √©l√©ments.
- `title`: La cl√© du titre de l'√©l√©ment.
- `url`: La cl√© de l'URL de l'√©l√©ment. Vous pouvez utiliser `{{nom_de_la_cle}}` pour substituer une valeur.
- `content`: Une liste de cl√©s s√©par√©es par des virgules pour le contenu.
- `image`: La cl√© de l'URL de l'image principale de l'√©l√©ment.

## 5. Lancer les Tests

Pour ex√©cuter la suite de tests, installez d'abord les d√©pendances de d√©veloppement:

```bash
pip install pytest
```

Ensuite, lancez les tests:
```bash
pytest
```
