# API KidSearch & Meilisearch Crawler

Ce projet fournit une solution backend compl√®te pour un moteur de recherche s√©curis√© comme [KidSearch](https://github.com/laurentftech/kidsearch). Il se compose de deux √©l√©ments principaux :

1.  Un **Serveur API KidSearch** : Un serveur bas√© sur FastAPI qui effectue des recherches f√©d√©r√©es sur plusieurs sources (Meilisearch local, Google, Wikipedia) et utilise un mod√®le local pour reclasser s√©mantiquement les r√©sultats.
2.  Un **Crawler Meilisearch** : Un crawler web asynchrone et performant qui peuple l'instance Meilisearch locale avec du contenu provenant de sites web, d'API JSON et de sites MediaWiki.

Cette combinaison cr√©e un backend de recherche puissant et flexible, capable de fournir des r√©sultats pertinents et s√ªrs.

## ‚ú® Fonctionnalit√©s

### Serveur API KidSearch
- **Backend FastAPI**: Un serveur d'API l√©ger et performant pour exposer les fonctionnalit√©s de recherche.
- **Recherche F√©d√©r√©e**: Agr√®ge en temps r√©el les r√©sultats de plusieurs sources : l'index Meilisearch local, Google Custom Search (GSE) et les API Wikipedia/Vikidia.
- **Reclassement Hybride Optimis√©**: R√©cup√®re les r√©sultats de toutes les sources, calcule les embeddings manquants √† la vol√©e, puis utilise un mod√®le *cross-encoder* local pour reclasser intelligemment la liste combin√©e en fonction de la pertinence s√©mantique. Cette approche garantit que le meilleur contenu est toujours prioris√© avec une latence minimale.
- **Pr√™t pour la Production**: Peut √™tre facilement d√©ploy√© en tant que conteneur Docker.

### C≈ìur du Crawler
- **Asynchrone & Parall√®le**: Con√ßu avec `asyncio` et `aiohttp` pour un crawl simultan√© √† haute vitesse.
- **Sources Flexibles**: Prend en charge nativement le crawl de sites web HTML, d'API JSON et de sites sous MediaWiki (comme Wikipedia ou Vikidia).
- **Crawl Incr√©mentiel**: Utilise un cache local pour ne r√©indexer que les pages qui ont chang√© depuis le dernier crawl, √©conomisant temps et ressources.
- **Reprise du Crawl**: Si un crawl est interrompu, il peut √™tre repris sans effort.
- **Extraction de Contenu Intelligente**: Utilise `trafilatura` pour une d√©tection robuste du contenu principal depuis le HTML.
- **Respect de `robots.txt`**: Suit les protocoles d'exclusion standards.
- **Exploration en Profondeur (Depth-First)**: Priorise l'exploration des liens nouvellement d√©couverts pour explorer plus profond√©ment la structure d'un site.

### Recherche & Indexation
- **Pr√™t pour la Recherche S√©mantique**: Peut g√©n√©rer et indexer des vecteurs d'embeddings en utilisant Google Gemini ou un mod√®le HuggingFace local.
- **Gestion de Quota Intelligente**: D√©tecte automatiquement lorsque le quota de l'API Gemini est d√©pass√© et arr√™te le crawl proprement.

### Supervision & Contr√¥le
- **Tableau de Bord Interactif**: Une interface web bas√©e sur Streamlit pour surveiller, contr√¥ler et configurer le crawler en temps r√©el.
- **CLI Avanc√©e**: Options de ligne de commande puissantes pour un contr√¥le pr√©cis.

![screenshot_dashboard.png](media/screenshot_dashboard_fr.png)

## Pr√©requis

- Python 3.8+
- Une instance Meilisearch en cours d'ex√©cution (v1.0 ou sup√©rieure).
- Une cl√© API Google Gemini (si vous utilisez la fonction d'embeddings).

## 1. Configuration de Meilisearch

Ce crawler a besoin d'une instance Meilisearch pour y envoyer ses donn√©es. La mani√®re la plus simple d'en obtenir une est avec Docker.

1.  **Installez Meilisearch**: Suivez le [guide de d√©marrage rapide officiel de Meilisearch](https://www.meilisearch.com/docs/learn/getting_started/quick_start).
2.  **Lancez Meilisearch avec une cl√© principale**:
    ```bash
    docker run -it --rm \
      -p 7700:7700 \
      -e MEILI_MASTER_KEY='une_cle_maitre_longue_et_securisee' \
      -v $(pwd)/meili_data:/meili_data \
      ghcr.io/meilisearch/meilisearch:latest
    ```
3.  **Obtenez votre URL et votre cl√© API**:
    -   **URL**: `http://localhost:7700`
    -   **Cl√© API**: La `MEILI_MASTER_KEY` que vous avez d√©finie.

## 2. Configuration du Crawler

1.  **Clonez le d√©p√¥t**:
    ```bash
    git clone https://github.com/laurentftech/MeilisearchCrawler.git
    cd MeilisearchCrawler
    ```

2.  **Cr√©ez et activez un environnement virtuel**:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Installez les d√©pendances**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configurez les variables d'environnement**:
    Copiez le fichier d'exemple et modifiez-le avec vos informations.
    ```bash
    cp .env.example .env
    ```
    Maintenant, ouvrez `.env` et remplissez :
    - `MEILI_URL`: L'URL de votre instance Meilisearch.
    - `MEILI_KEY`: Votre cl√© principale Meilisearch.
    - `GEMINI_API_KEY`: Votre cl√© API Google Gemini (optionnelle, mais requise pour l'option `--embeddings`).

5.  **Configurez les sites √† crawler**:
    Copiez le fichier d'exemple des sites.
    ```bash
    cp config/sites.yml.example config/sites.yml
    ```
    Vous pouvez maintenant modifier `config/sites.yml` pour ajouter les sites que vous souhaitez indexer.

## 3. Lancer l'Application

Le projet peut √™tre ex√©cut√© selon diff√©rents modes : crawler, serveur API ou tableau de bord.

> üìñ **Documentation compl√®te de l'API disponible ici :** [API_README_FR.md](API_README_FR.md)

### Crawler (Ligne de Commande)

Ex√©cutez le script `crawler.py` pour d√©marrer l'indexation du contenu.

```sh
python crawler.py # Lance un crawl incr√©mentiel sur tous les sites
```

**Options courantes:**

-   `--force`: Force une r√©indexation compl√®te de toutes les pages, en ignorant le cache.
-   `--site "Nom du Site"`: N'explore que le site sp√©cifi√©.
-   `--embeddings`: Active la g√©n√©ration d'embeddings Gemini pour la recherche s√©mantique.
-   `--workers N`: D√©finit le nombre de requ√™tes parall√®les (ex: `--workers 10`).
-   `--stats-only`: Affiche les statistiques du cache sans lancer de crawl.

**Exemple:**
```sh
# Force une r√©indexation de "Vikidia" avec les embeddings activ√©s
python crawler.py --force --site "Vikidia" --embeddings
```

### Serveur API KidSearch

Ex√©cutez le script `api.py` pour d√©marrer le serveur FastAPI, qui expose le point de terminaison de recherche.

```sh
python api.py
```
L'API sera disponible √† l'adresse `http://localhost:8000`. Vous pouvez acc√©der √† la documentation interactive √† l'adresse `http://localhost:8000/docs`.

### Tableau de Bord Interactif

Le projet inclut un tableau de bord web pour surveiller et contr√¥ler le crawler en temps r√©el.

**Comment le lancer:**

1.  Depuis la racine du projet, ex√©cutez la commande suivante:
    ```bash
    streamlit run dashboard/dashboard.py
    ```
2.  Ouvrez votre navigateur web √† l'URL locale fournie par Streamlit (g√©n√©ralement `http://localhost:8501`).

**Fonctionnalit√©s:**

-   **üè† Vue d'ensemble**: Un r√©sum√© en temps r√©el du crawl en cours.
-   **üîß Contr√¥les**: D√©marrez ou arr√™tez le crawler, s√©lectionnez des sites, forcez une r√©indexation et g√©rez les embeddings.
-   **üîç Recherche**: Une interface pour tester des requ√™tes directement sur votre index Meilisearch.
-   **üìä Statistiques**: Des statistiques d√©taill√©es sur votre index Meilisearch.
-   **üå≥ Arbre des Pages**: Une visualisation interactive de la structure de votre site.
-   **‚öôÔ∏è Configuration**: Un √©diteur interactif pour le fichier `sites.yml`.
-   **ü™µ Logs**: Une vue en direct du fichier de log du crawler.
-   **üìà M√©triques de l'API**: Un tableau de bord pour surveiller les performances et les m√©triques de l'API.

## 4. Configuration de `sites.yml`

Le fichier `config/sites.yml` vous permet de d√©finir une liste de sites √† crawler. Chaque site est un objet avec les propri√©t√©s suivantes:

- `name`: (String) Le nom du site, utilis√© pour le filtrage dans Meilisearch.
- `crawl`: (String) L'URL de d√©part pour le crawl.
- `type`: (String) Le type de contenu. Peut √™tre `html`, `json`, ou `mediawiki`.
- `max_pages`: (Integer) Le nombre maximum de pages √† crawler. Mettre `0` ou omettre pour ne pas avoir de limite.
- `depth`: (Integer) Pour les sites `html`, la profondeur maximale pour suivre les liens.
- `delay`: (Float, optionnel) Un d√©lai sp√©cifique en secondes entre les requ√™tes pour ce site, ignorant le d√©lai par d√©faut. Utile pour les serveurs sensibles.
- `selector`: (String, optionnel) Pour les sites `html`, un s√©lecteur CSS sp√©cifique (ex: `.main-article`) pour cibler le contenu principal.
- `lang`: (String, optionnel) Pour les sources `json`, sp√©cifie la langue du contenu (ex: "fr").
- `exclude`: (Liste de cha√Ænes) Une liste de motifs d'URL √† ignorer compl√®tement.
- `no_index`: (Liste de cha√Ænes) Une liste de motifs d'URL √† visiter pour d√©couvrir des liens mais √† ne pas indexer.

### Type `html`
C'est le type standard pour crawler des sites web classiques. Le crawler commencera √† l'URL `crawl` et suivra les liens jusqu'√† la `depth` sp√©cifi√©e.

### Type `json`
Pour ce type, vous devez fournir un objet `json` avec le mappage suivant:
- `root`: La cl√© dans la r√©ponse JSON qui contient la liste des √©l√©ments.
- `title`: La cl√© du titre de l'√©l√©ment.
- `url`: Un mod√®le pour l'URL de l'√©l√©ment. Vous pouvez utiliser `{{nom_de_la_cle}}` pour substituer une valeur de l'√©l√©ment.
- `content`: Une liste de cl√©s s√©par√©es par des virgules pour le contenu.
- `image`: La cl√© de l'URL de l'image principale.

### Type `mediawiki`
Ce type est optimis√© pour les sites utilisant le logiciel MediaWiki (comme Wikipedia, Vikidia). Il utilise l'API MediaWiki pour r√©cup√©rer efficacement toutes les pages, √©vitant un crawl lien par lien.
- L'URL `crawl` doit √™tre l'URL de base du wiki (ex: `https://fr.vikidia.org`).
- `depth` et `selector` ne sont pas utilis√©s pour ce type.

## 5. Authentification du Dashboard

### üß© Authentification KidSearch

KidSearch supporte nativement tout fournisseur d'identit√© compatible **OpenID Connect (OIDC)**, tel que :

- üîê **Pocket ID** (recommand√© pour usage self-hosted)
- üõ°Ô∏è **Authentik** (pour environnements multi-utilisateurs)
- üîµ **Google OAuth** (connexion avec comptes Google)
- ‚ö´ **GitHub OAuth** (connexion avec comptes GitHub)
- üîë **Mot de passe simple** (authentification basique)

### Configuration OIDC (Recommand√©)

Pour tout fournisseur OIDC standard (Pocket ID, Authentik, Keycloak, etc.), fournissez simplement les variables suivantes dans votre `.env` :

```bash
OIDC_ISSUER=https://auth.example.com
OIDC_CLIENT_ID=votre_client_id
OIDC_CLIENT_SECRET=votre_client_secret
OIDC_REDIRECT_URI=http://localhost:8501/
```

Les endpoints OIDC (authorization, token, userinfo) sont d√©couverts automatiquement via `/.well-known/openid-configuration`.

### Configuration de l'authentification OAuth

Pour activer l'authentification OAuth, configurez les variables suivantes dans votre fichier `.env` :

**Pour Google OAuth :**
```bash
GOOGLE_OAUTH_CLIENT_ID=votre_client_id.apps.googleusercontent.com
GOOGLE_OAUTH_CLIENT_SECRET=votre_client_secret
GOOGLE_OAUTH_REDIRECT_URI=http://localhost:8501/
ALLOWED_EMAILS=utilisateur1@gmail.com,utilisateur2@exemple.com
```

Obtenez les credentials sur : https://console.cloud.google.com/apis/credentials

**Pour GitHub OAuth :**
```bash
GITHUB_OAUTH_CLIENT_ID=votre_github_client_id
GITHUB_OAUTH_CLIENT_SECRET=votre_github_client_secret
GITHUB_OAUTH_REDIRECT_URI=http://localhost:8501/
ALLOWED_EMAILS=utilisateur1@exemple.com,utilisateur2@societe.com
```

Obtenez les credentials sur : https://github.com/settings/developers

### Liste d'emails autoris√©s

La variable `ALLOWED_EMAILS` restreint l'acc√®s √† des adresses email sp√©cifiques :
- Si vide : tous les utilisateurs authentifi√©s peuvent acc√©der (non recommand√© en production)
- Si configur√©e : seuls les emails list√©s peuvent acc√©der au dashboard

### Diagnostic des probl√®mes d'authentification

Si vous rencontrez des difficult√©s avec la connexion OAuth, utilisez les outils de diagnostic :

**1. V√©rifier votre configuration :**
```bash
python3 check_auth_config.py
```

Cela affichera :
- Quels providers d'authentification sont configur√©s
- Si vos credentials sont d√©finis
- Si un email est autoris√© √† acc√©der

**2. Tester un email sp√©cifique :**
```bash
python3 check_auth_config.py utilisateur@exemple.com
```

**3. Surveiller les logs d'authentification :**
```bash
./watch_auth_logs.sh
```

Ou consulter les logs directement :
```bash
tail -f data/logs/auth.log
```

Les logs afficheront :
- ‚úÖ Connexions r√©ussies avec les adresses email
- ‚ùå Connexions √©chou√©es avec raisons d√©taill√©es (email non autoris√©, email manquant, etc.)
- üîç D√©tails complets de la r√©ponse OAuth (en mode DEBUG)

## 6. Lancer les Tests

Pour ex√©cuter la suite de tests, installez d'abord les d√©pendances de d√©veloppement:

```bash
pip install pytest
```

Ensuite, lancez les tests:
```bash
pytest
```
