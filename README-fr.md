# Meilisearch Crawler

Ce projet est un crawler web asynchrone et performant, con√ßu pour peupler une instance Meilisearch avec le contenu de divers sites web. Il sert de compagnon au projet [KidSearch](https://github.com/laurentftech/kidsearch), un moteur de recherche s√©curis√© pour les enfants.

Le crawler est configurable via un simple fichier YAML (`sites.yml`) et prend en charge les pages HTML, les API JSON et les sites MediaWiki.

## ‚ú® Fonctionnalit√©s

- **Asynchrone & Parall√®le**: Con√ßu avec `asyncio` et `aiohttp` pour un crawl simultan√© √† haute vitesse.
- **Pr√™t pour la Recherche S√©mantique**: Peut g√©n√©rer et indexer des vecteurs d'embeddings avec l'API Google Gemini pour une recherche s√©mantique de pointe.
- **Gestion de Quota Intelligente**: D√©tecte automatiquement lorsque le quota de l'API Gemini est d√©pass√© et arr√™te le crawl proprement.
- **Tableau de Bord Interactif**: Une interface web bas√©e sur Streamlit pour surveiller, contr√¥ler et configurer le crawler en temps r√©el.
- **Sources Flexibles**: Prend en charge nativement le crawl de sites web HTML, d'API JSON et de sites sous MediaWiki (comme Wikipedia ou Vikidia).
- **Crawl Incr√©mentiel**: Utilise un cache local pour ne r√©indexer que les pages qui ont chang√© depuis le dernier crawl, √©conomisant temps et ressources.
- **Reprise du Crawl**: Si un crawl est interrompu (manuellement ou par une limite de pages), il peut √™tre repris sans effort.
- **Extraction de Contenu Intelligente**: Utilise `trafilatura` pour une d√©tection robuste du contenu principal depuis le HTML.
- **Respect de `robots.txt`**: Suit les protocoles d'exclusion standards.
- **CLI Avanc√©e**: Options de ligne de commande puissantes pour un contr√¥le pr√©cis.

![screenshot_dashboard.png](media/screenshot_dashboard_fr.png)

## Pr√©requis

- Python 3.8+
- Une instance Meilisearch en cours d'ex√©cution (v1.0 ou sup√©rieure).
- Une cl√© API Google Gemini (si vous utilisez la fonction d'embeddings).

## 1. Configuration de Meilisearch

Ce crawler a besoin d'une instance Meilisearch pour y envoyer ses donn√©es. La mani√®re la plus simple d'en obtenir une est avec Docker.

1.  **Installez Meilisearch**: Suivez le [guide de d√©marrage rapide officiel de Meilisearch](https://www.meilisearch.com/docs/learn/getting_started/quick_start).
2.  **Lancez Meilisearch avec une cl√© principale**:
    ```bash
    docker run -it --rm \
      -p 7700:7700 \
      -e MEILI_MASTER_KEY='une_cle_maitre_longue_et_securisee' \
      -v $(pwd)/meili_data:/meili_data \
      ghcr.io/meilisearch/meilisearch:latest
    ```
3.  **Obtenez votre URL et votre cl√© API**:
    -   **URL**: `http://localhost:7700`
    -   **Cl√© API**: La `MEILI_MASTER_KEY` que vous avez d√©finie.

## 2. Configuration du Crawler

1.  **Clonez le d√©p√¥t**:
    ```bash
    git clone https://github.com/laurentftech/MeilisearchCrawler.git
    cd MeilisearchCrawler
    ```

2.  **Cr√©ez et activez un environnement virtuel**:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Installez les d√©pendances**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configurez les variables d'environnement**:
    Copiez le fichier d'exemple et modifiez-le avec vos informations.
    ```bash
    cp .env.example .env
    ```
    Maintenant, ouvrez `.env` et remplissez :
    - `MEILI_URL`: L'URL de votre instance Meilisearch.
    - `MEILI_KEY`: Votre cl√© principale Meilisearch.
    - `GEMINI_API_KEY`: Votre cl√© API Google Gemini (optionnelle, mais requise pour l'option `--embeddings`).

5.  **Configurez les sites √† crawler**:
    Copiez le fichier d'exemple des sites.
    ```bash
    cp config/sites.yml.example config/sites.yml
    ```
    Vous pouvez maintenant modifier `config/sites.yml` pour ajouter les sites que vous souhaitez indexer.

## 3. Lancer le Crawler

Vous pouvez lancer le crawler via la ligne de commande ou le tableau de bord interactif.

### Interface en Ligne de Commande

Ex√©cutez simplement le script `crawler.py`:

```sh
python crawler.py # Lance un crawl incr√©mentiel sur tous les sites
```

**Options courantes:**

-   `--force`: Force une r√©indexation compl√®te de toutes les pages, en ignorant le cache.
-   `--site "Nom du Site"`: N'explore que le site sp√©cifi√©.
-   `--embeddings`: Active la g√©n√©ration d'embeddings Gemini pour la recherche s√©mantique.
-   `--workers N`: D√©finit le nombre de requ√™tes parall√®les (ex: `--workers 10`).
-   `--stats-only`: Affiche les statistiques du cache sans lancer de crawl.

**Exemple:**
```sh
# Force une r√©indexation de "Vikidia" avec les embeddings activ√©s
python crawler.py --force --site "Vikidia" --embeddings
```

### Tableau de Bord Interactif

Le projet inclut un tableau de bord web pour surveiller et contr√¥ler le crawler en temps r√©el.

**Comment le lancer:**

1.  Depuis la racine du projet, ex√©cutez la commande suivante:
    ```bash
    streamlit run dashboard/dashboard.py
    ```
2.  Ouvrez votre navigateur web √† l'URL locale fournie par Streamlit (g√©n√©ralement `http://localhost:8501`).

**Fonctionnalit√©s:**

-   **üè† Vue d'ensemble**: Un r√©sum√© en temps r√©el du crawl en cours.
-   **üîß Contr√¥les**: D√©marrez ou arr√™tez le crawler, s√©lectionnez des sites, forcez une r√©indexation et g√©rez les embeddings.
-   **üîç Recherche**: Une interface pour tester des requ√™tes directement sur votre index Meilisearch.
-   **üìä Statistiques**: Des statistiques d√©taill√©es sur votre index Meilisearch.
-   **üå≥ Arbre des Pages**: Une visualisation interactive de la structure de votre site.
-   **‚öôÔ∏è Configuration**: Un √©diteur interactif pour le fichier `sites.yml`.
-   **ü™µ Logs**: Une vue en direct du fichier de log du crawler.

## 4. Configuration de `sites.yml`

Le fichier `config/sites.yml` vous permet de d√©finir une liste de sites √† crawler. Chaque site est un objet avec les propri√©t√©s suivantes:

- `name`: (String) Le nom du site, utilis√© pour le filtrage dans Meilisearch.
- `crawl`: (String) L'URL de d√©part pour le crawl.
- `type`: (String) Le type de contenu. Peut √™tre `html`, `json`, ou `mediawiki`.
- `max_pages`: (Integer) Le nombre maximum de pages √† crawler. Mettre `0` ou omettre pour ne pas avoir de limite.
- `depth`: (Integer) Pour les sites `html`, la profondeur maximale pour suivre les liens.
- `delay`: (Float, optionnel) Un d√©lai sp√©cifique en secondes entre les requ√™tes pour ce site, ignorant le d√©lai par d√©faut. Utile pour les serveurs sensibles.
- `selector`: (String, optionnel) Pour les sites `html`, un s√©lecteur CSS sp√©cifique (ex: `.main-article`) pour cibler le contenu principal.
- `lang`: (String, optionnel) Pour les sources `json`, sp√©cifie la langue du contenu (ex: "fr").
- `exclude`: (Liste de cha√Ænes) Une liste de motifs d'URL √† ignorer compl√®tement.
- `no_index`: (Liste de cha√Ænes) Une liste de motifs d'URL √† visiter pour d√©couvrir des liens mais √† ne pas indexer.

### Type `html`
C'est le type standard pour crawler des sites web classiques. Le crawler commencera √† l'URL `crawl` et suivra les liens jusqu'√† la `depth` sp√©cifi√©e.

### Type `json`
Pour ce type, vous devez fournir un objet `json` avec le mappage suivant:
- `root`: La cl√© dans la r√©ponse JSON qui contient la liste des √©l√©ments.
- `title`: La cl√© du titre de l'√©l√©ment.
- `url`: Un mod√®le pour l'URL de l'√©l√©ment. Vous pouvez utiliser `{{nom_de_la_cle}}` pour substituer une valeur de l'√©l√©ment.
- `content`: Une liste de cl√©s s√©par√©es par des virgules pour le contenu.
- `image`: La cl√© de l'URL de l'image principale.

### Type `mediawiki`
Ce type est optimis√© pour les sites utilisant le logiciel MediaWiki (comme Wikipedia, Vikidia). Il utilise l'API MediaWiki pour r√©cup√©rer efficacement toutes les pages, √©vitant un crawl lien par lien.
- L'URL `crawl` doit √™tre l'URL de base du wiki (ex: `https://fr.vikidia.org`).
- `depth` et `selector` ne sont pas utilis√©s pour ce type.

## 5. Lancer les Tests

Pour ex√©cuter la suite de tests, installez d'abord les d√©pendances de d√©veloppement:

```bash
pip install pytest
```

Ensuite, lancez les tests:
```bash
pytest
```
